<<<<<<< HEAD
# Official Implementation for ICLR'2025 Paper

## [ICLR'2025] Rethinking Bradley-Terry Models in Preference-Based Reward Modeling: Foundations, Theory, and Alternatives

#### Authors: Hao Sun*, Yunyi Shen*, Jean-Francois Ton. The first two authors contribute equally.

[ [Website] ](https://sites.google.com/view/rewardmodels)        |      [ [Preprint] ](https://arxiv.org/pdf/2411.04991)       |       [Embeddings (To be released soon)]     |     [Code (To be released soon)]

## Infra for Easy-Reproducible Reward Model Research
The reproduction for reward modeling research has long been a challenge, given its high demand on hardware and cost in training, evaluation, and inference. We propose to conduct easy-reproducible reward model research on the embedding space.

Details of the workflow are posited in this paper [TO BE RELEASED SOON.]. Our motivation is to make every researcher with a single CPU can also conduct reward modeling (and RLHF) research.

## Reproducing the Results with a CPU
- Step 1 (optional, GPU required): SFT
- Step 2 (optional, GPU required): Generate samples on training (10 per prompt) and testing prompts (500 per prompt) 
- Step 3 (optional, GPU required): annotating response qualities using golden reward models
- Step 4 (optional, GPU required): Generate and store embeddings of all prompt-response pairs
- Step 5 



## Call for Contribution to the Infra (an Embedding-based Dataset for Reward Modeling Research)

`Call for contributors! --- Please contact me at sunhopht@gmail.com if your are interested in contributing your embedding / golden-reward annotations in your reward model research to the open-source RM community!`





=======
# Repo for Paper
#### Rethinking Bradley-Terry Models in Preference-Based Reward Modeling: Foundations, Theory, and Alternatives
#### Hao Sun*, Yunyi Shen*, Jean-Francois Ton

[ [Website] ](https://sites.google.com/view/rewardmodels)        |      [ [Preprint] ](https://arxiv.org/pdf/2411.04991)       |       [Embeddings (To be released soon)]     |     [Code (To be released soon)]


#### 
>>>>>>> 0ee0a90c81b9aab24ff4b8489ee1f214825051ca
