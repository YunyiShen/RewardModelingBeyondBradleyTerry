{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REP_MACRO 0 BETA 1.0 LOCAL_THRES 0.1\n",
      "REP_MACRO 0 BETA 1.0 LOCAL_THRES 0.2\n",
      "REP_MACRO 0 BETA 1.0 LOCAL_THRES 0.3\n",
      "REP_MACRO 0 BETA 1.0 LOCAL_THRES 0.4\n",
      "REP_MACRO 0 BETA 1.0 LOCAL_THRES 0.5\n",
      "REP_MACRO 0 BETA 1.0 LOCAL_THRES 0.6\n",
      "REP_MACRO 0 BETA 1.0 LOCAL_THRES 0.7\n",
      "REP_MACRO 0 BETA 1.0 LOCAL_THRES 0.8\n",
      "REP_MACRO 0 BETA 1.0 LOCAL_THRES 0.9\n",
      "REP_MACRO 0 BETA 1.0 LOCAL_THRES 1.0\n",
      "REP_MACRO 0 BETA 10.0 LOCAL_THRES 0.1\n",
      "REP_MACRO 0 BETA 10.0 LOCAL_THRES 0.2\n",
      "REP_MACRO 0 BETA 10.0 LOCAL_THRES 0.3\n",
      "REP_MACRO 0 BETA 10.0 LOCAL_THRES 0.4\n",
      "REP_MACRO 0 BETA 10.0 LOCAL_THRES 0.5\n",
      "REP_MACRO 0 BETA 10.0 LOCAL_THRES 0.6\n",
      "REP_MACRO 0 BETA 10.0 LOCAL_THRES 0.7\n",
      "REP_MACRO 0 BETA 10.0 LOCAL_THRES 0.8\n",
      "REP_MACRO 0 BETA 10.0 LOCAL_THRES 0.9\n",
      "REP_MACRO 0 BETA 10.0 LOCAL_THRES 1.0\n",
      "REP_MACRO 1 BETA 1.0 LOCAL_THRES 0.1\n",
      "REP_MACRO 1 BETA 1.0 LOCAL_THRES 0.2\n",
      "REP_MACRO 1 BETA 1.0 LOCAL_THRES 0.3\n",
      "REP_MACRO 1 BETA 1.0 LOCAL_THRES 0.4\n",
      "REP_MACRO 1 BETA 1.0 LOCAL_THRES 0.5\n",
      "REP_MACRO 1 BETA 1.0 LOCAL_THRES 0.6\n",
      "REP_MACRO 1 BETA 1.0 LOCAL_THRES 0.7\n",
      "REP_MACRO 1 BETA 1.0 LOCAL_THRES 0.8\n",
      "REP_MACRO 1 BETA 1.0 LOCAL_THRES 0.9\n",
      "REP_MACRO 1 BETA 1.0 LOCAL_THRES 1.0\n",
      "REP_MACRO 1 BETA 10.0 LOCAL_THRES 0.1\n",
      "REP_MACRO 1 BETA 10.0 LOCAL_THRES 0.2\n",
      "REP_MACRO 1 BETA 10.0 LOCAL_THRES 0.3\n",
      "REP_MACRO 1 BETA 10.0 LOCAL_THRES 0.4\n",
      "REP_MACRO 1 BETA 10.0 LOCAL_THRES 0.5\n",
      "REP_MACRO 1 BETA 10.0 LOCAL_THRES 0.6\n",
      "REP_MACRO 1 BETA 10.0 LOCAL_THRES 0.7\n",
      "REP_MACRO 1 BETA 10.0 LOCAL_THRES 0.8\n",
      "REP_MACRO 1 BETA 10.0 LOCAL_THRES 0.9\n",
      "REP_MACRO 1 BETA 10.0 LOCAL_THRES 1.0\n",
      "REP_MACRO 2 BETA 1.0 LOCAL_THRES 0.1\n",
      "REP_MACRO 2 BETA 1.0 LOCAL_THRES 0.2\n",
      "REP_MACRO 2 BETA 1.0 LOCAL_THRES 0.3\n",
      "REP_MACRO 2 BETA 1.0 LOCAL_THRES 0.4\n",
      "REP_MACRO 2 BETA 1.0 LOCAL_THRES 0.5\n",
      "REP_MACRO 2 BETA 1.0 LOCAL_THRES 0.6\n",
      "REP_MACRO 2 BETA 1.0 LOCAL_THRES 0.7\n",
      "REP_MACRO 2 BETA 1.0 LOCAL_THRES 0.8\n",
      "REP_MACRO 2 BETA 1.0 LOCAL_THRES 0.9\n",
      "REP_MACRO 2 BETA 1.0 LOCAL_THRES 1.0\n",
      "REP_MACRO 2 BETA 10.0 LOCAL_THRES 0.1\n",
      "REP_MACRO 2 BETA 10.0 LOCAL_THRES 0.2\n",
      "REP_MACRO 2 BETA 10.0 LOCAL_THRES 0.3\n",
      "REP_MACRO 2 BETA 10.0 LOCAL_THRES 0.4\n",
      "REP_MACRO 2 BETA 10.0 LOCAL_THRES 0.5\n",
      "REP_MACRO 2 BETA 10.0 LOCAL_THRES 0.6\n",
      "REP_MACRO 2 BETA 10.0 LOCAL_THRES 0.7\n",
      "REP_MACRO 2 BETA 10.0 LOCAL_THRES 0.8\n",
      "REP_MACRO 2 BETA 10.0 LOCAL_THRES 0.9\n",
      "REP_MACRO 2 BETA 10.0 LOCAL_THRES 1.0\n",
      "REP_MACRO 3 BETA 1.0 LOCAL_THRES 0.1\n",
      "REP_MACRO 3 BETA 1.0 LOCAL_THRES 0.2\n",
      "REP_MACRO 3 BETA 1.0 LOCAL_THRES 0.3\n",
      "REP_MACRO 3 BETA 1.0 LOCAL_THRES 0.4\n",
      "REP_MACRO 3 BETA 1.0 LOCAL_THRES 0.5\n",
      "REP_MACRO 3 BETA 1.0 LOCAL_THRES 0.6\n",
      "REP_MACRO 3 BETA 1.0 LOCAL_THRES 0.7\n",
      "REP_MACRO 3 BETA 1.0 LOCAL_THRES 0.8\n",
      "REP_MACRO 3 BETA 1.0 LOCAL_THRES 0.9\n",
      "REP_MACRO 3 BETA 1.0 LOCAL_THRES 1.0\n",
      "REP_MACRO 3 BETA 10.0 LOCAL_THRES 0.1\n",
      "REP_MACRO 3 BETA 10.0 LOCAL_THRES 0.2\n",
      "REP_MACRO 3 BETA 10.0 LOCAL_THRES 0.3\n",
      "REP_MACRO 3 BETA 10.0 LOCAL_THRES 0.4\n",
      "REP_MACRO 3 BETA 10.0 LOCAL_THRES 0.5\n",
      "REP_MACRO 3 BETA 10.0 LOCAL_THRES 0.6\n",
      "REP_MACRO 3 BETA 10.0 LOCAL_THRES 0.7\n",
      "REP_MACRO 3 BETA 10.0 LOCAL_THRES 0.8\n",
      "REP_MACRO 3 BETA 10.0 LOCAL_THRES 0.9\n",
      "REP_MACRO 3 BETA 10.0 LOCAL_THRES 1.0\n",
      "REP_MACRO 4 BETA 1.0 LOCAL_THRES 0.1\n",
      "REP_MACRO 4 BETA 1.0 LOCAL_THRES 0.2\n",
      "REP_MACRO 4 BETA 1.0 LOCAL_THRES 0.3\n",
      "REP_MACRO 4 BETA 1.0 LOCAL_THRES 0.4\n",
      "REP_MACRO 4 BETA 1.0 LOCAL_THRES 0.5\n",
      "REP_MACRO 4 BETA 1.0 LOCAL_THRES 0.6\n",
      "REP_MACRO 4 BETA 1.0 LOCAL_THRES 0.7\n",
      "REP_MACRO 4 BETA 1.0 LOCAL_THRES 0.8\n",
      "REP_MACRO 4 BETA 1.0 LOCAL_THRES 0.9\n",
      "REP_MACRO 4 BETA 1.0 LOCAL_THRES 1.0\n",
      "REP_MACRO 4 BETA 10.0 LOCAL_THRES 0.1\n",
      "REP_MACRO 4 BETA 10.0 LOCAL_THRES 0.2\n",
      "REP_MACRO 4 BETA 10.0 LOCAL_THRES 0.3\n",
      "REP_MACRO 4 BETA 10.0 LOCAL_THRES 0.4\n",
      "REP_MACRO 4 BETA 10.0 LOCAL_THRES 0.5\n",
      "REP_MACRO 4 BETA 10.0 LOCAL_THRES 0.6\n",
      "REP_MACRO 4 BETA 10.0 LOCAL_THRES 0.7\n",
      "REP_MACRO 4 BETA 10.0 LOCAL_THRES 0.8\n",
      "REP_MACRO 4 BETA 10.0 LOCAL_THRES 0.9\n",
      "REP_MACRO 4 BETA 10.0 LOCAL_THRES 1.0\n",
      "REP_MACRO 5 BETA 1.0 LOCAL_THRES 0.1\n",
      "REP_MACRO 5 BETA 1.0 LOCAL_THRES 0.2\n",
      "REP_MACRO 5 BETA 1.0 LOCAL_THRES 0.3\n",
      "REP_MACRO 5 BETA 1.0 LOCAL_THRES 0.4\n",
      "REP_MACRO 5 BETA 1.0 LOCAL_THRES 0.5\n",
      "REP_MACRO 5 BETA 1.0 LOCAL_THRES 0.6\n",
      "REP_MACRO 5 BETA 1.0 LOCAL_THRES 0.7\n",
      "REP_MACRO 5 BETA 1.0 LOCAL_THRES 0.8\n",
      "REP_MACRO 5 BETA 1.0 LOCAL_THRES 0.9\n",
      "REP_MACRO 5 BETA 1.0 LOCAL_THRES 1.0\n",
      "REP_MACRO 5 BETA 10.0 LOCAL_THRES 0.1\n",
      "REP_MACRO 5 BETA 10.0 LOCAL_THRES 0.2\n",
      "REP_MACRO 5 BETA 10.0 LOCAL_THRES 0.3\n",
      "REP_MACRO 5 BETA 10.0 LOCAL_THRES 0.4\n",
      "REP_MACRO 5 BETA 10.0 LOCAL_THRES 0.5\n",
      "REP_MACRO 5 BETA 10.0 LOCAL_THRES 0.6\n",
      "REP_MACRO 5 BETA 10.0 LOCAL_THRES 0.7\n",
      "REP_MACRO 5 BETA 10.0 LOCAL_THRES 0.8\n",
      "REP_MACRO 5 BETA 10.0 LOCAL_THRES 0.9\n",
      "REP_MACRO 5 BETA 10.0 LOCAL_THRES 1.0\n",
      "REP_MACRO 6 BETA 1.0 LOCAL_THRES 0.1\n",
      "REP_MACRO 6 BETA 1.0 LOCAL_THRES 0.2\n",
      "REP_MACRO 6 BETA 1.0 LOCAL_THRES 0.3\n",
      "REP_MACRO 6 BETA 1.0 LOCAL_THRES 0.4\n",
      "REP_MACRO 6 BETA 1.0 LOCAL_THRES 0.5\n",
      "REP_MACRO 6 BETA 1.0 LOCAL_THRES 0.6\n",
      "REP_MACRO 6 BETA 1.0 LOCAL_THRES 0.7\n",
      "REP_MACRO 6 BETA 1.0 LOCAL_THRES 0.8\n",
      "REP_MACRO 6 BETA 1.0 LOCAL_THRES 0.9\n",
      "REP_MACRO 6 BETA 1.0 LOCAL_THRES 1.0\n",
      "REP_MACRO 6 BETA 10.0 LOCAL_THRES 0.1\n",
      "REP_MACRO 6 BETA 10.0 LOCAL_THRES 0.2\n",
      "REP_MACRO 6 BETA 10.0 LOCAL_THRES 0.3\n",
      "REP_MACRO 6 BETA 10.0 LOCAL_THRES 0.4\n",
      "REP_MACRO 6 BETA 10.0 LOCAL_THRES 0.5\n",
      "REP_MACRO 6 BETA 10.0 LOCAL_THRES 0.6\n",
      "REP_MACRO 6 BETA 10.0 LOCAL_THRES 0.7\n",
      "REP_MACRO 6 BETA 10.0 LOCAL_THRES 0.8\n",
      "REP_MACRO 6 BETA 10.0 LOCAL_THRES 0.9\n",
      "REP_MACRO 6 BETA 10.0 LOCAL_THRES 1.0\n",
      "REP_MACRO 7 BETA 1.0 LOCAL_THRES 0.1\n",
      "REP_MACRO 7 BETA 1.0 LOCAL_THRES 0.2\n",
      "REP_MACRO 7 BETA 1.0 LOCAL_THRES 0.3\n",
      "REP_MACRO 7 BETA 1.0 LOCAL_THRES 0.4\n",
      "REP_MACRO 7 BETA 1.0 LOCAL_THRES 0.5\n",
      "REP_MACRO 7 BETA 1.0 LOCAL_THRES 0.6\n",
      "REP_MACRO 7 BETA 1.0 LOCAL_THRES 0.7\n",
      "REP_MACRO 7 BETA 1.0 LOCAL_THRES 0.8\n",
      "REP_MACRO 7 BETA 1.0 LOCAL_THRES 0.9\n",
      "REP_MACRO 7 BETA 1.0 LOCAL_THRES 1.0\n",
      "REP_MACRO 7 BETA 10.0 LOCAL_THRES 0.1\n",
      "REP_MACRO 7 BETA 10.0 LOCAL_THRES 0.2\n",
      "REP_MACRO 7 BETA 10.0 LOCAL_THRES 0.3\n",
      "REP_MACRO 7 BETA 10.0 LOCAL_THRES 0.4\n",
      "REP_MACRO 7 BETA 10.0 LOCAL_THRES 0.5\n",
      "REP_MACRO 7 BETA 10.0 LOCAL_THRES 0.6\n",
      "REP_MACRO 7 BETA 10.0 LOCAL_THRES 0.7\n",
      "REP_MACRO 7 BETA 10.0 LOCAL_THRES 0.8\n",
      "REP_MACRO 7 BETA 10.0 LOCAL_THRES 0.9\n",
      "REP_MACRO 7 BETA 10.0 LOCAL_THRES 1.0\n",
      "REP_MACRO 8 BETA 1.0 LOCAL_THRES 0.1\n",
      "REP_MACRO 8 BETA 1.0 LOCAL_THRES 0.2\n",
      "REP_MACRO 8 BETA 1.0 LOCAL_THRES 0.3\n",
      "REP_MACRO 8 BETA 1.0 LOCAL_THRES 0.4\n",
      "REP_MACRO 8 BETA 1.0 LOCAL_THRES 0.5\n",
      "REP_MACRO 8 BETA 1.0 LOCAL_THRES 0.6\n",
      "REP_MACRO 8 BETA 1.0 LOCAL_THRES 0.7\n",
      "REP_MACRO 8 BETA 1.0 LOCAL_THRES 0.8\n",
      "REP_MACRO 8 BETA 1.0 LOCAL_THRES 0.9\n",
      "REP_MACRO 8 BETA 1.0 LOCAL_THRES 1.0\n",
      "REP_MACRO 8 BETA 10.0 LOCAL_THRES 0.1\n",
      "REP_MACRO 8 BETA 10.0 LOCAL_THRES 0.2\n",
      "REP_MACRO 8 BETA 10.0 LOCAL_THRES 0.3\n",
      "REP_MACRO 8 BETA 10.0 LOCAL_THRES 0.4\n",
      "REP_MACRO 8 BETA 10.0 LOCAL_THRES 0.5\n",
      "REP_MACRO 8 BETA 10.0 LOCAL_THRES 0.6\n",
      "REP_MACRO 8 BETA 10.0 LOCAL_THRES 0.7\n",
      "REP_MACRO 8 BETA 10.0 LOCAL_THRES 0.8\n",
      "REP_MACRO 8 BETA 10.0 LOCAL_THRES 0.9\n",
      "REP_MACRO 8 BETA 10.0 LOCAL_THRES 1.0\n",
      "REP_MACRO 9 BETA 1.0 LOCAL_THRES 0.1\n",
      "REP_MACRO 9 BETA 1.0 LOCAL_THRES 0.2\n",
      "REP_MACRO 9 BETA 1.0 LOCAL_THRES 0.3\n",
      "REP_MACRO 9 BETA 1.0 LOCAL_THRES 0.4\n",
      "REP_MACRO 9 BETA 1.0 LOCAL_THRES 0.5\n",
      "REP_MACRO 9 BETA 1.0 LOCAL_THRES 0.6\n",
      "REP_MACRO 9 BETA 1.0 LOCAL_THRES 0.7\n",
      "REP_MACRO 9 BETA 1.0 LOCAL_THRES 0.8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 129\u001b[0m\n\u001b[1;32m    126\u001b[0m lgb_train \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(X_train, y_train)\n\u001b[1;32m    127\u001b[0m lgb_eval \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(X_test, y_test, reference\u001b[38;5;241m=\u001b[39mlgb_train)\n\u001b[0;32m--> 129\u001b[0m lgb_model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mtrain({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_logloss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m}, lgb_train, valid_sets\u001b[38;5;241m=\u001b[39mlgb_eval, num_boost_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m    131\u001b[0m y_pred_lgb \u001b[38;5;241m=\u001b[39m lgb_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m    132\u001b[0m y_pred_lgb \u001b[38;5;241m=\u001b[39m (y_pred_lgb \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/lightgbm/engine.py:307\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    296\u001b[0m     cb(\n\u001b[1;32m    297\u001b[0m         callback\u001b[38;5;241m.\u001b[39mCallbackEnv(\n\u001b[1;32m    298\u001b[0m             model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m         )\n\u001b[1;32m    305\u001b[0m     )\n\u001b[0;32m--> 307\u001b[0m booster\u001b[38;5;241m.\u001b[39mupdate(fobj\u001b[38;5;241m=\u001b[39mfobj)\n\u001b[1;32m    309\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/lightgbm/basic.py:4136\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   4133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   4134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4135\u001b[0m _safe_call(\n\u001b[0;32m-> 4136\u001b[0m     _LIB\u001b[38;5;241m.\u001b[39mLGBM_BoosterUpdateOneIter(\n\u001b[1;32m   4137\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle,\n\u001b[1;32m   4138\u001b[0m         ctypes\u001b[38;5;241m.\u001b[39mbyref(is_finished),\n\u001b[1;32m   4139\u001b[0m     )\n\u001b[1;32m   4140\u001b[0m )\n\u001b[1;32m   4141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[1;32m   4142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# more complex task:\n",
    "\n",
    "# create a synthetic regression task:\n",
    "\n",
    "def func_form(input_x):\n",
    "    return np.cos(7*input_x+0.5) + 3 * input_x - 0.4 * (input_x+0.5)**3\n",
    "\n",
    "def cognitive_bottleneck_comparison(y1, y2, beta):\n",
    "    thres = np.exp(beta * (y1 - y2)) / (1 + np.exp(beta * (y1 - y2)))\n",
    "    if np.random.rand() < thres:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "VISUALIZE = False\n",
    "XLIM =2\n",
    "H1 = 100\n",
    "H2 = 100\n",
    "# LOCAL_THRES = 0.1\n",
    "\n",
    "macro_out_list = []\n",
    "for REP_MACRO in range(30):\n",
    "    final_out_dict = {}\n",
    "    for BETA in [1.0, 10.0]:\n",
    "        final_out_dict[BETA] = {}\n",
    "        for LOCAL_THRES in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "            print(f\"REP_MACRO {REP_MACRO} BETA {BETA} LOCAL_THRES {LOCAL_THRES}\")\n",
    "            final_out_dict[BETA][LOCAL_THRES] = {}\n",
    "            import numpy as np\n",
    "            import matplotlib.pyplot as plt\n",
    "\n",
    "            out_dict = {}\n",
    "            for N_EPOCH in [50]: #\n",
    "\n",
    "                out_dict[N_EPOCH] = {}\n",
    "                final_out_dict[BETA][LOCAL_THRES][N_EPOCH] = {}\n",
    "                if VISUALIZE:\n",
    "                    fig, axes = plt.subplots(4, 4, figsize=(40, 40))\n",
    "                for j, N_SAMPLE in enumerate([50, 100, 1000, 3000, 5000]):\n",
    "                    for k, N_COMPARE in enumerate([50, 100, 1000, 3000, 5000]):\n",
    "                        out_dict[N_EPOCH][str((N_SAMPLE, N_COMPARE))] = {}\n",
    "                        final_out_dict[BETA][LOCAL_THRES][N_EPOCH][str((N_SAMPLE, N_COMPARE))] = {}\n",
    "                        if VISUALIZE:\n",
    "                            ax = axes[j, k]\n",
    "\n",
    "                        X = np.random.rand(N_SAMPLE, 1) * XLIM\n",
    "                        # y = 2 + 3 * X + np.random.randn(N_SAMPLE, 1)\n",
    "                        y =  func_form(X)\n",
    "\n",
    "                        #normalize y to 0-1\n",
    "                        # ymax = y.max()\n",
    "                        # ymin = y.min()\n",
    "\n",
    "                        # y = (y - y.min()) / (y.max() - y.min())\n",
    "                        # y = (y - y.mean())  / y.std()\n",
    "\n",
    "                        # plt.plot(X, y, \"b.\")\n",
    "                        # plt.xlabel(\"$x$\", fontsize=18)\n",
    "                        # plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "                        # draw the true line after normalization\n",
    "                        # plt.plot(X, (2 + 3 * X - ymin)/(ymax-ymin), \"r\")\n",
    "\n",
    "\n",
    "                        # create the pairwise dataset by randomly pick two for comparison:\n",
    "\n",
    "\n",
    "\n",
    "                        pairwise_dataset = []\n",
    "                        pairwise_label = []\n",
    "                        while len(pairwise_dataset) < N_COMPARE:\n",
    "                            # locally sample pairwise data\n",
    "                            idx1 = np.random.randint(0, N_SAMPLE)\n",
    "                            idx2 = np.random.randint(0, N_SAMPLE)\n",
    "                            if np.abs(X[idx1] - X[idx2]) > LOCAL_THRES:\n",
    "                                continue\n",
    "                            pairwise_dataset.append((X[idx1].item(), X[idx2].item()))\n",
    "                            # pairwise_label.append(1 if y[idx1] > y[idx2] else 0)\n",
    "                            pairwise_label.append(cognitive_bottleneck_comparison(y[idx1], y[idx2], BETA))\n",
    "                        # for i in range(N_COMPARE):\n",
    "                        #     # locally sample pairwise data\n",
    "                        #     while\n",
    "                        #     idx1 = np.random.randint(0, N_SAMPLE)\n",
    "                        #     idx2 = np.random.randint(0, N_SAMPLE)\n",
    "                        #     pairwise_dataset.append((X[idx1].item(), X[idx2].item()))\n",
    "                        #     pairwise_label.append(1 if y[idx1] > y[idx2] else 0)\n",
    "\n",
    "                        pairwise_dataset = np.array(pairwise_dataset)\n",
    "                        pairwise_label = np.array(pairwise_label)\n",
    "\n",
    "\n",
    "\n",
    "                        clf_dataset = []\n",
    "                        clf_label = []\n",
    "                        for i, (x1, x2) in enumerate(pairwise_dataset):\n",
    "                            if pairwise_label[i] == 1:\n",
    "                                clf_dataset.append(x1)\n",
    "                                clf_label.append(1)\n",
    "                                clf_dataset.append(x2)\n",
    "                                clf_label.append(0)\n",
    "                            else:\n",
    "                                clf_dataset.append(x1)\n",
    "                                clf_label.append(0)\n",
    "                                clf_dataset.append(x2)\n",
    "                                clf_label.append(1)\n",
    "\n",
    "                        clf_dataset = np.array(clf_dataset).reshape(-1, 1)\n",
    "                        clf_label = np.array(clf_label)\n",
    "\n",
    "                        # flip the pairwise data to augment the dataset\n",
    "                        pairwise_dataset = np.vstack([pairwise_dataset, pairwise_dataset[:, ::-1]])\n",
    "                        pairwise_label = np.hstack([pairwise_label, 1 - pairwise_label])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        # logistic regression\n",
    "                        from sklearn.linear_model import LogisticRegression\n",
    "                        from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "                        X_train, X_test, y_train, y_test = train_test_split(clf_dataset, clf_label, test_size=0.2)\n",
    "                        X_train_siamese, X_test_siamese, y_train_siamese, y_test_siamese = train_test_split(pairwise_dataset, pairwise_label, test_size=0.2)\n",
    "                        # train tree based model\n",
    "                        import lightgbm as lgb\n",
    "\n",
    "                        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "                        lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "                        lgb_model = lgb.train({'objective': 'binary', 'metric': 'binary_logloss', 'verbosity': -1}, lgb_train, valid_sets=lgb_eval, num_boost_round=1000)\n",
    "\n",
    "                        y_pred_lgb = lgb_model.predict(X_test)\n",
    "                        y_pred_lgb = (y_pred_lgb > 0.5).astype(int)\n",
    "\n",
    "                        # print(\"Train accuracy: \", log_reg.score(X_train, y_train))\n",
    "                        # print(\"Test accuracy: \", log_reg.score(X_test, y_test))\n",
    "\n",
    "                        # mlp model\n",
    "                        from networks import MLP, forward_mlp, forward_siamese, train_model\n",
    "                        import torch\n",
    "                        import torch.optim as optim\n",
    "                        import torch.nn as nn\n",
    "\n",
    "                        X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "                        X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "                        y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "                        y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                        X_train_siamese = torch.tensor(X_train_siamese, dtype=torch.float32).unsqueeze(-1)\n",
    "                        X_test_siamese = torch.tensor(X_test_siamese, dtype=torch.float32).unsqueeze(-1)\n",
    "                        y_train_siamese = torch.tensor(y_train_siamese, dtype=torch.float32).unsqueeze(1)\n",
    "                        y_test_siamese = torch.tensor(y_test_siamese, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                        mlp = MLP(1, H1, H2)\n",
    "                        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                        train_model(mlp, device, 'clf', X_train, y_train, X_test, y_test, N_EPOCH, 0.001)\n",
    "\n",
    "                        # eval the model\n",
    "                        mlp.eval()\n",
    "                        y_logits = forward_mlp(mlp, X_test)\n",
    "                        y_logits = y_logits.detach().numpy()\n",
    "                        y_pred = (y_logits > 0.5).astype(int)\n",
    "                        # print(\"Test accuracy CLF: \", np.mean(y_pred == y_test.numpy()))\n",
    "\n",
    "                        bt = MLP(1, H1, H2)\n",
    "                        train_model(bt, device, 'siamese', X_train_siamese, y_train_siamese, X_test_siamese, y_test_siamese, N_EPOCH, 0.001)\n",
    "                        bt.eval()\n",
    "                        y_logits_bt = forward_siamese(bt, X_test_siamese[:, 0], X_test_siamese[:, 1])\n",
    "                        y_logits_bt = y_logits_bt.detach().numpy()\n",
    "                        y_pred_bt = (y_logits_bt > 0.5).astype(int)\n",
    "                        # print(\"Test accuracy BT: \", np.mean(y_pred_bt == y_test_siamese.numpy()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        if VISUALIZE:\n",
    "                            # predict the probability of any given data\n",
    "                            X_new = np.linspace(0, XLIM, 1000).reshape(-1, 1)\n",
    "                            # y_proba = log_reg.predict_proba(X_new)\n",
    "                            y_mlp_logits = mlp(torch.tensor(X_new, dtype=torch.float32)).detach().numpy()\n",
    "                            y_bt_logits = bt(torch.tensor(X_new, dtype=torch.float32)).detach().numpy()\n",
    "\n",
    "\n",
    "                            # plt.plot(X_train, y_train, \"b.\")\n",
    "                            # plt.plot(X_test, y_test, \"g.\")\n",
    "                            ax.scatter(X, y, c='gray',alpha=0.3)\n",
    "                            ax.set_xlabel(\"$x$\", fontsize=18)\n",
    "                            ax.set_ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "                            # draw the true line after normalization\n",
    "\n",
    "                            line_dots = np.linspace(0, XLIM, 1000).reshape(-1, 1)\n",
    "                            ax.plot(line_dots, func_form(line_dots), \"r\", label=\"True\")\n",
    "                            # draw the true functional curve\n",
    "\n",
    "                            ax.plot(X_new, y_proba[:, 1], \"g-\", label=\"CLF - Logistic Regression\")\n",
    "                            ax.plot(X_new, y_mlp_logits, \"b-\", label=\"CLF - MLP\")\n",
    "                            ax.plot(X_new, y_bt_logits, \"c-\", label=\"BT - MLP\")\n",
    "                            ax.set_title(f\"N_SAMPLE = {N_SAMPLE}, N_COMPARE = {N_COMPARE} EPOCH = {N_EPOCH} Diversity {LOCAL_THRES} Beta {BETA}\")\n",
    "                            ax.legend()\n",
    "                            # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        # using best of N to evaluate the proxy reward models\n",
    "                        bon_mlp_list = []\n",
    "                        bon_bt_list = []\n",
    "                        bon_lgb_list = []\n",
    "                        for repeat in range(10):\n",
    "                            mlp_temp_list = []\n",
    "                            bt_temp_list = []\n",
    "                            lgb_temp_list = []\n",
    "                            for bon_n in [2, 5, 10, 50, 100, 300, 500]:\n",
    "                                X_bon = np.random.rand(bon_n, 1) * XLIM\n",
    "                                y_bon = func_form(X_bon) #+ np.random.randn(bon_n, 1)\n",
    "\n",
    "                                # use different reward model to evaluate the performance\n",
    "                                # 1. logistic regression\n",
    "                                y_bon_mlp_logits = mlp(torch.tensor(X_bon, dtype=torch.float32)).detach().numpy()\n",
    "                                y_bon_bt_logits = bt(torch.tensor(X_bon, dtype=torch.float32)).detach().numpy()\n",
    "                                y_bon_lgb_logits = lgb_model.predict(X_bon)\n",
    "                                # argmax the reward model\n",
    "                                mlp_argmax_score = y_bon[np.argmax(y_bon_mlp_logits)]\n",
    "                                bt_argmax_score = y_bon[np.argmax(y_bon_bt_logits)]\n",
    "                                lgb_argmax_score = y_bon[np.argmax(y_bon_lgb_logits)]\n",
    "                                # print(f\"best of {bon_n} CLF argmax: {mlp_argmax_score}, BT argmax: {bt_argmax_score}\")\n",
    "                                mlp_temp_list.append(mlp_argmax_score.item())\n",
    "                                bt_temp_list.append(bt_argmax_score.item())\n",
    "                                lgb_temp_list.append(lgb_argmax_score.item())\n",
    "                            bon_mlp_list.append(mlp_temp_list)\n",
    "                            bon_bt_list.append(bt_temp_list)\n",
    "                            bon_lgb_list.append(lgb_temp_list)\n",
    "\n",
    "\n",
    "                        out_dict[N_EPOCH][str((N_SAMPLE, N_COMPARE))]['clf-bon-'] = bon_mlp_list\n",
    "                        out_dict[N_EPOCH][str((N_SAMPLE, N_COMPARE))]['bt-bon-'] = bon_bt_list\n",
    "                        out_dict[N_EPOCH][str((N_SAMPLE, N_COMPARE))]['lgb-bon-'] = bon_lgb_list\n",
    "                        final_out_dict[BETA][LOCAL_THRES][N_EPOCH][str((N_SAMPLE, N_COMPARE))]['clf-bon'] = np.asarray(bon_mlp_list).mean(0)[-1]\n",
    "                        final_out_dict[BETA][LOCAL_THRES][N_EPOCH][str((N_SAMPLE, N_COMPARE))]['bt-bon'] = np.asarray(bon_bt_list).mean(0)[-1]\n",
    "                        final_out_dict[BETA][LOCAL_THRES][N_EPOCH][str((N_SAMPLE, N_COMPARE))]['lgb-bon'] = np.asarray(bon_lgb_list).mean(0)[-1]\n",
    "                        if VISUALIZE:\n",
    "                            x_list = np.arange(7) * 0.16\n",
    "                            # use dual axis to plot the distribution of the best of N\n",
    "                            ax2 = ax.twinx()\n",
    "                            ax2.plot(x_list, np.mean(bon_mlp_list, 0), color = 'orange')\n",
    "                            ax2.errorbar(x_list, np.mean(bon_mlp_list, 0), yerr=np.std(bon_mlp_list, 0), fmt='o', color='orange')\n",
    "                            ax2.plot(x_list, np.mean(bon_bt_list, 0), color='black')\n",
    "                            ax2.errorbar(x_list, np.mean(bon_bt_list, 0), yerr=np.std(bon_bt_list, 0), fmt='o', color='black')\n",
    "                            ax2.set_ylabel(\"Golden Score Best of N\", fontsize=18)\n",
    "                            ax2.set_ylim(-4, 4)\n",
    "                            ax2.legend(['CLF', 'BT'])\n",
    "\n",
    "\n",
    "                # plt.savefig(f\"synthetic_{N_EPOCH}_diverse_beta{BETA}.png\", dpi=300, bbox_inches='tight')\n",
    "                if VISUALIZE:\n",
    "                    plt.show()\n",
    "\n",
    "    macro_out_list.append(final_out_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for betavalue in [1.0, 10.0]:\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(30, 30))\n",
    "    idexer = 0\n",
    "    for i in [50, 100, 1000, 3000, 5000]:\n",
    "        for j in [50, 100, 1000, 3000, 5000]:\n",
    "            ax = axes[idexer//5, idexer%5]\n",
    "            idexer += 1\n",
    "            list_of_clf_list = []\n",
    "            list_of_bt_list = []\n",
    "            list_of_lgb_list = []\n",
    "            for REPEAT in range(30):\n",
    "                clf_list = []\n",
    "                bt_list = []\n",
    "                lgb_list = []\n",
    "                for k in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "                    clf_list.append(macro_out_list[REPEAT][betavalue][k][50][str((i, j))]['clf-bon'])\n",
    "                    bt_list.append(macro_out_list[REPEAT][betavalue][k][50][str((i, j))]['bt-bon'])\n",
    "                    lgb_list.append(macro_out_list[REPEAT][betavalue][k][50][str((i, j))]['lgb-bon'])\n",
    "                list_of_clf_list.append(clf_list)\n",
    "                list_of_bt_list.append(bt_list)\n",
    "                list_of_lgb_list.append(lgb_list)\n",
    "            final_clf_list = np.mean(list_of_clf_list, 0)\n",
    "            final_bt_list = np.mean(list_of_bt_list, 0)\n",
    "            final_clf_std = np.std(list_of_clf_list, 0)\n",
    "            final_bt_std = np.std(list_of_bt_list, 0)\n",
    "            final_lgb_list = np.mean(list_of_lgb_list, 0)\n",
    "            final_lgb_std = np.std(list_of_lgb_list, 0)\n",
    "            ax.plot(np.arange(1, 11)*0.1, final_clf_list, 'r', label='CLF')\n",
    "            ax.fill_between(np.arange(1, 11)*0.1, final_clf_list - final_clf_std, final_clf_list + final_clf_std, color='r', alpha=0.3)\n",
    "            ax.plot(np.arange(1, 11)*0.1, final_bt_list, 'b', label='BT')\n",
    "            ax.fill_between(np.arange(1, 11)*0.1, final_bt_list - final_bt_std, final_bt_list + final_bt_std, color='b', alpha=0.3)\n",
    "            ax.plot(np.arange(1, 11)*0.1, final_lgb_list, 'g', label='LGB')\n",
    "            ax.fill_between(np.arange(1, 11)*0.1, final_lgb_list - final_lgb_std, final_lgb_list + final_lgb_std, color='g', alpha=0.3)\n",
    "\n",
    "            ax.set_title(f\"N_SAMPLE = {i}, N_COMPARE = {j} Beta {betavalue}\")\n",
    "            ax.set_xlabel(\"Diversity\")\n",
    "            ax.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# XLIM =1\n",
    "# H1 = 50\n",
    "# H2 = 50\n",
    "# # LOCAL_THRES = 0.1\n",
    "\n",
    "# macro_out_list = []\n",
    "# for REP_MACRO in range(30):\n",
    "#     final_out_dict = {}\n",
    "#     for BETA in [1.0, 10.0]:\n",
    "#         final_out_dict[BETA] = {}\n",
    "#         for LOCAL_THRES in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "#             print(f\"REP_MACRO {REP_MACRO} BETA {BETA} LOCAL_THRES {LOCAL_THRES}\")\n",
    "#             final_out_dict[BETA][LOCAL_THRES] = {}\n",
    "#             import numpy as np\n",
    "#             import matplotlib.pyplot as plt\n",
    "\n",
    "#             out_dict = {}\n",
    "#             for N_EPOCH in [50]: #\n",
    "\n",
    "# save to json file\n",
    "import json\n",
    "import os\n",
    "os.makedirs('macro_out', exist_ok=True)\n",
    "for REP_MACRO in range(30):\n",
    "    with open(f'macro_out/macro_out_{REP_MACRO}_epoch{N_EPOCH}_nonlinearity-{XLIM}-large.json', 'w') as f:\n",
    "        json.dump(macro_out_list[REP_MACRO], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(macro_out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str((N_SAMPLE, N_COMPARE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
